#+TITLE: Chapter 1. Gaining Early Insights from Textual Data
#+PROPERTY: header-args :tangle insights.py

* Libaries

#+BEGIN_SRC python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
#+END_SRC

* Read the Data

#+BEGIN_SRC python
un = pd.read_csv("data/un-debates.csv")

print(un)
#+END_SRC

* Blueprint: Getting an Overview of the Data with Pandas

We will perform the following steps:

1. Calculate summary statistics
2. Check for missing values
3. Plot distributions of interesting attributes
4. Compare distributions across categories
5. Visualize developments over time

#+BEGIN_SRC python
# print column names
print(un.columns.tolist())

# print column data types
print(un.dtypes)

# data types plus memory consumption
print(un.info())

# summary statistics
print(un.describe())

# summary statistics for all columns (including categorical)
print(un.describe())
#+END_SRC

** Calculating Summary Statistics for Columns

Let's add a new numerical column to the dataframe containing the text length to get some additional information about the distribution of the lengths of the speeches.

#+BEGIN_SRC python
# add text length column and describe it
un = un.assign(length = un["text"].str.len())

print(un.assign(length=un["text"].str.len()).describe().T)
print(un.assign(length=un["text"].str.len()).describe(include="O").T)

# check number of unique values for categorical predictors
print(un[["country", "speaker"]].describe(include="O").T)
#+END_SRC

** Checking for Missing Data

#+BEGIN_SRC python
# check how many NA values we have
print(un.isna().sum())

# fill in the NA with "unknown"
# warning: this is a mutable operation
print(un["speaker"].fillna("unknown", inplace = True))
print(un.isna().sum())

# check specific values and their counts
print(un[un["speaker"].str.contains("Bush")]["speaker"].value_counts())
#+END_SRC

** Plotting Value Distributions

#+BEGIN_SRC python
# plot a box and whisker plot and a histogram side by side
plt.subplot(1, 2, 1)
un["length"].plot(kind = "box", vert = False)
plt.subplot(1, 2, 2)
un["length"].plot(kind = "hist", bins = 30)
plt.title("Speech Length (Characters)")
plt.show()


def gen_dist_plot(column_name):
    plt.close()
    # plot a single distribution plot
    sns.displot(un[column_name], kind = "kde")
    sns.rugplot(un[column_name])
    plt.title(column_name.title())
    plt.show()

gen_dist_plot("length")
#+END_SRC

** Comparing Value Distributions Across Categories

A nice visualization to compare distributions across different categories is Seaborn's catplot

#+BEGIN_SRC python
where = un["country"].isin(["USA", "FRA", "GBR", "CHN", "RUS"])

print(un[where])

sns.catplot(data = un[where], x="country", y="length", kind="box")
sns.catplot(data = un[where], x="country", y="length", kind="violin")
plt.show()
#+END_SRC

** Visualizing Developments Over Time

#+BEGIN_SRC python
plt.subplot(1, 2, 1)
un.groupby("year").size().plot(title = "Number of Countries")
plt.subplot(1, 2, 2)
un.groupby("year").agg({"length": "mean"}).plot(title = "Avg Speech Length", ylim = (0, 30000))
plt.show()
#+END_SRC

#+BEGIN_SRC python :tangle ch1/overview.py
# import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


class Overview:
    """
    This class instantiates an object that provides an overview of a data frame.
    Example:

    >> overview = Overview(df)
    # get summary statistics
    >> overview.summary_stats()
    ## check for missing values
    >> overview.check_missing()
    ## generate a specific univariate plot
    >> overview.gen_uni_plot("column_name")
    ## generate all univariate plots
    >> overview.gen_all_unis()
    """

    def __init__(self, df):
        self.df = df

    def summary_stats(self, mem_usage="deep", include="O"):
        """
        Returns a dictionary containing the following summary stats:

        - col names: df.dtype
        - data types + memory consumption: df.info
          - set mem_usage to "" if you don't want to spend more time on "deeper" memory estimates
        - summary: df.describe
          - set include to "" if you don't wish to include categorical variables
        """
        column_names = list(self.df.columns)
        # returns a function. Evaluate to get info.
        ## This is because df.info is just a print side effect
        data_types = lambda: self.df.info(memory_usage=mem_usage)
        summary = self.df.describe(include=include).T

        return {
            "col_names": column_names,
            "data_types": data_types,
            "summary": summary,
        }

    def check_missing(self):
        """
        Returns the counts of missing values in the dataframe
        """
        return self.df.isna().sum()

    def gen_uni_plot(self, column_name):
        """
        Generates a univariate density plot for the given column name. Requires a numeric or datetime column
        """
        new_plot = UnivariatePlot(self.df, column_name)
        new_plot.gen_plot()

    def gen_all_unis(self):
        # the [:-1] is because the text field is too large to fix in the axis labels
        return [self.gen_uni_plot(i) for i in self.summary_stats()["col_names"][:-1]]


# un_overview = Overview(un)
# un_overview.gen_all_unis()


class UnivariatePlot:
    sns.set(palette="colorblind")

    def __init__(self, df, column_name, keep_null=False):
        self.column_name = column_name
        # if you wish to keep the null values, pass True to keep_null
        if keep_null:
            self.df = df[column_name].to_frame()
        else:
            self.df = df[column_name].dropna().to_frame()

    # def gen_dist_plot(self):
    #     """
    #     Generates a univariate density plot for the given column name. Requires a numeric or datetime column
    #     """
    #     plt.close()
    #     # plot a single distribution plot
    #     sns.displot(data=self.df, kind="kde")
    #     sns.rugplot(data=self.df)
    #     plt.title(self.column_name.title())
    #     plt.show()

    def gen_dist_plot_double(self):
        """
        Generates a pair of plots:
        - a box and whisker plot on the left
        - a histogram on the right
        """
        plt.subplot(1, 2, 1)
        self.df[self.column_name].plot(kind="box", vert=False)
        plt.title("Speech Length (Characters)")
        plt.subplot(1, 2, 2)
        self.df[self.column_name].plot(kind="hist", bins=30)
        plt.show()

    def gen_count_plot(self, top_n=10):
        """
        Generates a count plot for the given column name.
        Returns @top_n values ordered by highest cardinality
        """

        plt.close()
        sns.countplot(
            y=self.column_name,
            data=self.df,
            order=self.df[self.column_name].value_counts().iloc[:top_n].index,
        )
        plt.title(self.column_name.title())
        plt.show()

    def gen_plot(self):
        if self.df[self.column_name].dtype == "object":
            self.gen_count_plot()
        elif self.df[self.column_name].dtype in ["int64", "datetime", "float"]:
            self.gen_dist_plot()
        else:
            raise ValueError("Column type not in [object, int64, datetime, float]")


# un_len = UnivariatePlot(un, "length")
# un_position = UnivariatePlot(df=un, column_name="country")
# un_position.gen_plot()
# un_len.gen_plot()
# un_len.gen_dist_plot_double()
#+END_SRC

* Blueprint: Building a Simple Text Preprocessing Pipeline

Our pipeline will look something like this:

source text -> case-folding -> tokenization -> stop word removal -> prepared tokens

** Performing Tokenization with Regular Expressions

#+BEGIN_SRC python
import regex as re

def tokenize(text):
    # \p{L} matches all unicode letters
    return re.findall(r"[\w-]*\p{L}[\w-]*", text)

text = "Let's defeat SARS-CoV-2 together in 2020!"

print("|".join(tokens := tokenize(text)))
#+END_SRC

** Treating Stop Words

#+BEGIN_SRC python
stopwords = set(nltk.corpus.stopwords.words("english"))

def remove_stopwords(tokens):
    return [t for t in tokens if t.lower() not in stopwords]

# adding additional stopwords
include_stopwords = {"dear", "regards", "must", "would", "also"}
exclude_stopwords = {"against"}

stopwords |= include_stopwords
stopwords -= exclude_stopwords
#+END_SRC

** Processing a Pipeline with One Line of Code

#+BEGIN_SRC python
from toolz import compose, partial

pipeline = [str.lower, tokenize, remove_stopwords]

# lol
def prepare(text, pipeline):
    # reverses the pipeline and calls it in reverse-order on the text
    return compose(*pipeline[::-1])(text)

# applying prepare to a dataframe
un = un.assign(tokens = un["text"].apply(prepare, pipeline = pipeline),
               num_tokens = un["tokens"].map(len))

print(un[["text", "tokens", "num_tokens"]])
#+END_SRC

*** Pandas Higher Order Functions

| Function     | Description                                               |
|--------------+-----------------------------------------------------------|
| Series.map   | Works element by element on a Pandas Series               |
| Series.apply | Same as map but allows additional params                  |
| df.applymap  | rowwise map on a dataframe                                |
| df.apply     | works on rows or columns of a df and supports aggregation |

There is also the pandarallel package for performing operations on data frames in parallel

#+BEGIN_SRC python
from pandarallel import pandarallel
import math

pandarallel.initialize(progress_bar=True, verbose=2)

df_size = int(5e6)
df = pd.DataFrame({"a": np.random.randint(1, 8, df_size), "b": np.random.rand(df_size)})

# df.apply
res_parallel = df.parallel_apply(
    lambda x: math.sin(x.a ** 2) + math.sin(x.b ** 2), axis=1
)
print(res_parallel)

# series.map
df_size = int(1e7)
df = pd.DataFrame(dict(a=np.random.randint(1, 8, df_size), b=np.random.rand(df_size)))

def func(x):
    return math.sin(x ** 2) - math.cos(x ** 2)

df = df.assign(results = df["a"].parallel_map(func))

print(df)
#+END_SRC

* Blueprint: Counting Words with a Counter

#+BEGIN_SRC python
from collections import Counter

tokens = tokenize("She likes my cats and my cats like my sofa")

print(counter := Counter(tokens))

# update the counter
more_tokens = tokenize("She likes dogs and cats.")
counter.update(more_tokens)
print(counter)

# get the counts of all the tokens in the df
counter = Counter()

un["tokens"].map(counter.update)

print(counter.most_common(5))

# make the counter into a dataframe
def count_words(df, column = "tokens", preprocess = None, min_freq = 2):
    # process tokens and update counter
    def update(doc):
        tokens = doc if preprocess is None else preprocess(doc)
        counter.update(tokens)

    # create a counter and run through all data
    counter = Counter()
    df[column].map(update)

    # transform counter into a DataFrame
    freq_df = pd.DataFrame.from_dict(counter, orient = "index", columns = ["freq"]).query("freq >= @min_freq")
    freq_df.index.name = "token"

    return freq_df.sort_values("freq", ascending = False)

print(count_words(un).head(10))

print(count_words(un, column = "text", preprocess = partial(prepare, pipeline = pipeline)))
#+END_SRC

TextPreprocessor
- lower
- tokenize
- remove_stop
- get_word_frequencies
- get_word_tfidf
- frequency_plot
- word_cloud



* Blueprint: Creating a Frequency Diagram

#+BEGIN_SRC python
freq_df = count_words(un)

ax = freq_df.head(15).plot(kind = "barh", width = 0.95)
ax.invert_yaxis()
ax.set(xlabel = "Frequency", ylabel = "Token", title = "Top Words")
plt.show()
#+END_SRC

* Blueprint: Creating Word Clouds

#+BEGIN_SRC python
from wordcloud import WordCloud

# for one text
text = un.query("year == 2015 and country == 'USA'")["text"].values[0]

wc = WordCloud(max_words = 100, stopwords = stopwords)
wc.generate(text)
plt.imshow(wc, interpolation = "bilinear")
plt.axis("off")
plt.show()

def wordcloud(word_freq, title = None, max_words = 100, stopwords = None):
    # create word cloud
    wc = WordCloud(width = 800, height = 400, background_color = "white", colormap = "Paired", max_font_size = 150, max_words = max_words)

    # convert DF to dict
    if type(word_freq) == pd.Series:
        counter = Counter(word_freq.fillna(0).to_dict())
    else:
        counter = word_freq

    # filter stop words in frequency counter
    if stopwords is not None:
        counter = {token:freq for (token, freq) in counter.items() if token not in stopwords}

    plt.close()
    wc.generate_from_frequencies(counter)
    plt.title(title)
    plt.imshow(wc, interpolation = "bilinear")
    plt.axis("off")
    plt.show()

freq_2015_df = count_words(un.query("year == 2015"))

plt.figure()
wordcloud(freq_2015_df["freq"], max_words = 100)
wordcloud(freq_2015_df["freq"], max_words = 100, stopwords = freq_df.head(50).index)
plt.show()
#+END_SRC

* Blueprint: Ranking with TF-IDF

#+BEGIN_SRC python
def compute_idf(df, column = "tokens", preprocess = None, min_df = 2):
    def update(doc):
        tokens = doc if preprocess is None else preprocess(doc)
        counter.update(set(tokens))

    # count tokens
    counter = Counter()
    df[column].map(counter.update)

    # create data frame and compute idf
    idf_df = pd.DataFrame.from_dict(counter, orient = "index", columns = ["df"])
    idf_df["idf"] = np.log(len(df) / idf_df["df"]) + 0.1
    idf_df.index.name = "token"
    return idf_df

idf_df = compute_idf(un)

def attach_tfidf(df, idf_df = idf_df):
    return df.assign(tf_idf = df["freq"] * idf_df["idf"])


print(idf_df)

# get tf idf
freq_df["tfidf"] = freq_df["freq"] * idf_df["idf"]

print(freq_df)

# try out some word clouds with tf idf weighting
freq_1970 = attach_tfidf(count_words(un.query("year == 1970")))
freq_2015 = attach_tfidf(count_words(un.query("year == 2015")))

print(freq_1970)
print(freq_2015)

wordcloud(freq_1970["freq"], title = "1970 - TF",
          stopwords = ["twenty-fifth", "twenty-five"])
wordcloud(freq_2015["freq"], title = "2015 - TF",
          stopwords = ["seventieth"])
wordcloud(freq_1970["tf_idf"], title = "1970 - TF-IDF",
          stopwords = ["twenty-fifth", "twenty-five"])
wordcloud(freq_2015["tf_idf"], title = "2015 - TF-IDF",
          stopwords = ["seventieth"])
#+END_SRC
