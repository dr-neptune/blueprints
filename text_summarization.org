#+TITLE: Chapter 9: Text Summarization

* Extractive Methods

All extractive methods follow 3 basic steps:
1. Create an intermediate representation of the text
2. Score the sentences/phrases based on the chosen representation
3. Rank and choose sentences to create a summary of the text

* Data Preprocessing

#+BEGIN_SRC python
from bs4 import BeautifulSoup
import reprlib
import requests
import os
import re


r = reprlib.Repr()
r.maxstring = 800

url1 = "https://www.reuters.com/article/us-qualcomm-m-a-broadcom-5g/what-is-5g-and-who-are-the-major-players-idUSKCN1GR1IN"

def download_article(url):
    # check if article already there
    fname = url.split("/")[-1] + ".html"
    if not os.path.isfile(fname):
        r = requests.get(url)
        with open(fname, "w+") as f:
            f.write(r.text)
    return fname

def parse_article(article_file):
    with open(article_file, "r") as f:
        html = f.read()
    r = {}
    soup = BeautifulSoup(html, "html.parser")
    r["url"] = soup.find("link", {"rel": "canonical"})["href"]
    r["headline"] = soup.h1.text
    r["text"] = soup.select_one("div.ArticleBodyWrapper").text

    # clean up sentence endings
    r["text"] = re.sub("\\.", ". ", r["text"])
    r["text"] = re.sub("\\?", "? ", r["text"])

    return r

article_name1 = download_article(url1)

print(article_name1)

article1 = parse_article(article_name1)

#+END_SRC

* Identifying Important Words with TF-IDF Values

#+BEGIN_SRC python
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk import tokenize
import numpy as np


sentences = tokenize.sent_tokenize(article1['text'])
tfidfVectorizer = TfidfVectorizer()
words_tfidf = tfidfVectorizer.fit_transform(sentences)

# param to specify # of summary sentences required
num_summary_sentence = 3

# sort the sentences in descending order by the sum of TFIDF values
sent_sum = words_tfidf.sum(axis=1)
important_sent = np.argsort(sent_sum, axis=0)[::-1]

# print 3 most important sentences in the order they appear in article
for i in range(0, len(sentences)):
    if i in important_sent[:num_summary_sentence]:
        print(f"\n\nSentence {i}\n\n")
        print(sentences[i])

#+END_SRC

* Latent Semantic Analysis

LSA assumes that words that are close in meaning will occur in the same documents.
We essentially do a Non-negative Matrix Factorization and then generate the summary by choosing the
top N important topics and then picking the most important sentences for each of these topics to form our summary.

#+BEGIN_SRC python
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.nlp.stemmers import Stemmer
from sumy.utils import get_stop_words
from sumy.summarizers.lsa import LsaSummarizer

LANGUAGE = 'english'
stemmer = Stemmer(LANGUAGE)
parser = PlaintextParser.from_string(article1['text'],
                                     Tokenizer(LANGUAGE))
summarizer = LsaSummarizer(stemmer)
summarizer.stop_words = get_stop_words(LANGUAGE)

for sentence in summarizer(parser.document,
                           num_summary_sentence):
    print(str(sentence))
#+END_SRC

* Blueprint: Summarizing Text Using an Indicator Representation
