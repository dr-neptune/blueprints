#+TITLE: Chapter 9: Text Summarization

* Extractive Methods

All extractive methods follow 3 basic steps:
1. Create an intermediate representation of the text
2. Score the sentences/phrases based on the chosen representation
3. Rank and choose sentences to create a summary of the text

* Data Preprocessing

#+BEGIN_SRC python
from bs4 import BeautifulSoup
import reprlib
import requests
import os
import re


r = reprlib.Repr()
r.maxstring = 800

url1 = "https://www.reuters.com/article/us-qualcomm-m-a-broadcom-5g/what-is-5g-and-who-are-the-major-players-idUSKCN1GR1IN"

def download_article(url):
    # check if article already there
    fname = url.split("/")[-1] + ".html"
    if not os.path.isfile(fname):
        r = requests.get(url)
        with open(fname, "w+") as f:
            f.write(r.text)
    return fname

def parse_article(article_file):
    with open(article_file, "r") as f:
        html = f.read()
    r = {}
    soup = BeautifulSoup(html, "html.parser")
    r["url"] = soup.find("link", {"rel": "canonical"})["href"]
    r["headline"] = soup.h1.text
    r["text"] = soup.select_one("div.ArticleBodyWrapper").text

    # clean up sentence endings
    r["text"] = re.sub("\\.", ". ", r["text"])
    r["text"] = re.sub("\\?", "? ", r["text"])

    return r

article_name1 = download_article(url1)

print(article_name1)

article1 = parse_article(article_name1)

#+END_SRC

* Identifying Important Words with TF-IDF Values

#+BEGIN_SRC python
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk import tokenize
import numpy as np


sentences = tokenize.sent_tokenize(article1['text'])
tfidfVectorizer = TfidfVectorizer()
words_tfidf = tfidfVectorizer.fit_transform(sentences)

# param to specify # of summary sentences required
num_summary_sentence = 3

# sort the sentences in descending order by the sum of TFIDF values
sent_sum = words_tfidf.sum(axis=1)
important_sent = np.argsort(sent_sum, axis=0)[::-1]

# print 3 most important sentences in the order they appear in article
for i in range(0, len(sentences)):
    if i in important_sent[:num_summary_sentence]:
        print(f"\n\nSentence {i}\n\n")
        print(sentences[i])

#+END_SRC

* Latent Semantic Analysis

LSA assumes that words that are close in meaning will occur in the same documents.
We essentially do a Non-negative Matrix Factorization and then generate the summary by choosing the
top N important topics and then picking the most important sentences for each of these topics to form our summary.

#+BEGIN_SRC python
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.nlp.stemmers import Stemmer
from sumy.utils import get_stop_words
from sumy.summarizers.lsa import LsaSummarizer

LANGUAGE = 'english'
stemmer = Stemmer(LANGUAGE)
parser = PlaintextParser.from_string(article1['text'],
                                     Tokenizer(LANGUAGE))
summarizer = LsaSummarizer(stemmer)
summarizer.stop_words = get_stop_words(LANGUAGE)

for sentence in summarizer(parser.document,
                           num_summary_sentence):
    print(str(sentence))
#+END_SRC

* Blueprint: Summarizing Text Using an Indicator Representation

Indicator representation methods aim to create the intermediate representation of a sentence using features of the sentence and its relationship to others in the document rather than using only the words in the sentence.

#+BEGIN_SRC python
from sumy.summarizers.text_rank import TextRankSummarizer

parser = PlaintextParser.from_string(article1['text'],
                                     Tokenizer(LANGUAGE))
summarizer = TextRankSummarizer(stemmer)
summarizer.stop_words = get_stop_words(LANGUAGE)

for sentence in summarizer(parser.document, num_summary_sentence):
    print(str(sentence))
#+END_SRC

* Measuring the Performance of Text Summarization Methods

Recall-Oriented Understudy for Gisting Evaluation (ROUGE) is one of the most commonly used methods to measure the accuracy of a summary.

#+BEGIN_SRC python
from rouge import Rouge

rouge = Rouge()

scores = rouge.get_scores(hypothesis, reference)

gold_standard = article1['headline']
summary = ''.join([str(sentence) for sentence in summarizer(parser.document, num_summary_sentence)])

scores = rouge.get_scores(summary, gold_standard)
#+END_SRC

* Blueprint: Summarizing Text Using Machine Learning

** Step 1: Creating Target Labels

#+BEGIN_SRC python
import pandas as pd
import numpy as np
import textdistance
from sklearn.model_selection import GroupShuffleSplit


df = pd.read_csv('travel_threads.csv', sep='|', dtype={'ThreadID': 'object'})

# apply regex cleaning
df['text'] = df['text'].apply(regex_clean)

# extract lemmas
df['lemmas'] = df['text'].apply(clean)


# cv split
gss = GroupShuffleSplit(n_splits=1, test_size=0.2)
train_split, test_split = next(gss.split(df, groups=df['ThreadID']))
train_df, test_df = df.iloc[train_split], df.iloc[test_split]

print('Number of Threads for Training: ', train_df['ThreadID'].nunique())
print('Number of Threads for Testing: ', test_df['ThreadID'].nunique())

# measure text distance
compression_factor = 0.3

train_df['similarity'] = train_df.apply(lambda x: textdistance.jaro_winkler(x.text, x.summary), axis=1)
train_df['rank'] = train_df.groupby('ThreadID')['similarity'].rank('max', ascending=False)

topN = lambda x: x <= np.ceil(compression_factor * x.max())
train_df['summaryPost'] = train_df.groupby('ThreadID')['rank'].apply(topN)

train_df[['text', 'summaryPost']][train_df['ThreadID'] == '60763_5_3122150'].head(3)
#+END_SRC

** Step 2: Adding Features to Assist Model Prediction

#+BEGIN_SRC python
# calculate the difference between the post and the title as a feature
train_df['titleSimilarity'] = train_df.apply(lambda x: textdistance.jaro_winkler(x.text, x.Title), axis=1)

# adding post length as a feature
train_df['textLength'] = train_df['text'].str.len()

# add vectorized lemmas and additional features created earlier
feature_cols = ['titleSimilarity', 'textLength', 'postNum']

train_df['combined'] = [' '.join(map(str, l)) for l in train_df['lemmas'] if l is not '']
tfidf = TfidfVectorizer(min_df=10, ngram_range=(1, 2), stop_words="english")
tfidf_result = tfidf.fit_transform(train_df['combined']).toarray()
tfidf_df = pd.DataFrame(tfidf_result, columns=tfidf.get_feature_names())
tfidf_df.columns = ['word_' + str(x) for x in tfidf_df.columns]
tfidf_df.index = train_df.index
train_df_tf = pd.concat([train_df[feature_cols], tfidf_df], axis=1)
#+END_SRC

** Step 3: Building a Model

#+BEGIN_SRC python
from sklearn.ensemble import RandomForestClassifier

model1 = RandomForestClassifier()
model1.fit(train_df_tf, train_df['summaryPost'])

def calculate_rouge_score(x, column_name):
    # get the original summary - only first value since they are repeated
    ref_summary = x['summary'].values[0]

    # join all posts that have been predicted as summary
    predicted_summary = ''.join(x['text'][x[column_name]])

    # return the rouge score for each ThreadID
    scores = rouge.Rouge().get_scores(predicted_summary, ref_summary)

    return scores['rouge1'].fmeasure()

test_df['predictedSummaryPost'] = model1.predict(test_df_tf)
print('Mean ROUGE-1 Score for Test Threads', test_df.groupby('ThreadID')[['summary', 'text', 'predictedSummaryPost']].apply(calculate_rouge_score,
                                                                                                                            column_name='predictedSummaryPost').mean())
#+END_SRC
