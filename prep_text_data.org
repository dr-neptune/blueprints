#+TITLE: Preparing Textual Data for Statistics and Machine Learning

#+BEGIN_SRC python
import pandas as pd
from toolz.functoolz import compose

posts_file = "data/rspct_autos.tsv.gz"
posts_df = pd.read_csv(posts_file, sep = "\t")

subred_file = "data/subreddit_info.csv"
subred_df = pd.read_csv(subred_file).set_index(["subreddit"])

df = posts_df.join(subred_df, on = "subreddit")
#+END_SRC

* Blueprint: Standardizing Attribute Names

#+BEGIN_SRC python
print(df.columns)

column_mapping = {
    "id": "id",
    "subreddit": "subreddit",
    "title": "title",
    "selftext": "text",
    "category_1": "category",
    "category_2": "subcategory",
    "category_3": None,
    "in_data": None,
    "reason_for_exclusion": None
}

# define remaining columns
columns = [c for c in column_mapping.keys() if column_mapping[c] != None]

# select and rename those columns
df = df[columns].rename(columns = column_mapping)

# check that data is limited to autos
print(df.query("category == 'autos'"))

# sample a record to get a look at data
print(df.sample(1).T)

glimpse(df)

def glimpse(df):
    print(df.sample(1).T)

# saving and loading a dataframe
import sqlite3

# write to sqlite db
db_name = "reddit-selfposts.db"
con = sqlite3.connect(db_name)
df.to_sql("posts", con, index = False, if_exists = "replace")
con.close()

# restore from sqlite db
con = sqlite3.connect(db_name)
df = pd.read_sql("select * from posts", con)
con.close()
#+END_SRC

* Blueprint: Identify Noise with Regular Expressions

#+BEGIN_SRC python
import re

sus_text = """
After viewing the [PINKIEPOOL Trailer](https://www.youtu.be/watch?
v=ieHRoHUg)
it got me thinking about the best match ups.
<lb>Here's my take:<lb><lb>[](/sp)[](/ppseesyou) Deadpool<lb>[]
(/sp)[](/ajsly)
Captain America<lb>"""

sus_pattern = re.compile(r"[&#<>{}\[\]\\]")

def impurity(text, min_len = 10):
    """
    Returns the share of suspicious characters in a text
    """
    if text == None or len(text) < min_len:
        return 0
    else:
        return len(sus_pattern.findall(text)) / len(text)

print(impurity(sus_text))

# add new column to the data frame
print(df := df.assign(impurity = df["text"].apply(impurity, min_len = 10)))

# get the top 3 most impure records
print(df[["text", "impurity"]].sort_values(by = "impurity", ascending = False).head(3))

# get instances of <lb>, <tab>
from collections import Counter

def count_words(df, column = "tokens", preprocess = None, min_freq = 2):
    # process tokens and update counter
    def update(doc):
        tokens = doc if preprocess is None else preprocess(doc)
        counter.update(tokens)

    # create a counter and run through all data
    counter = Counter()
    df[column].map(update)

    # transform counter into a DataFrame
    freq_df = pd.DataFrame.from_dict(counter, orient = "index", columns = ["freq"]).query("freq >= @min_freq")
    freq_df.index.name = "token"

    return freq_df.sort_values("freq", ascending = False)

print(count_words(df, column = "text", preprocess=lambda t: re.findall(r"<[\w/]*>", t)))
#+END_SRC

* Blueprint: Removing Noise with Regular Expressions

#+BEGIN_SRC python
import html

def clean(text):
    # convert HTML escapes like &amp; to characters
    text = html.unescape(text)

    # chain of re.subs
    replacements = [
        # tags like <tab>, <lb>
        (r"<[^<>]*>", " "),
        # markdown URLs like [some text](https://...)
        (r"\[([^\[\]]*)\]\([^\(\)]*\)", r"\1"),
        # text or code in brackets like [0]
        (r"\[[^\[\]]*\]", " "),
        # standalone sequences of specials, matches &# but not #word
        (r"(?:^|\s)[&#<>{}\[\]+|\\:-]{1,}(?:\s|$)", " "),
        # standalone sequences of hyphens like --- or ==
        (r"(?:^|\s)[\-=\+]{2,}(?:\s|$)", " "),
        # sequences of white space
        (r"\s+", " ")
    ]

    # run through all the replacements
    for pattern, replacement in replacements:
        text = re.sub(pattern, replacement, text)

    return text.strip()

print(clean(sus_text))
print("Impurity:\t", impurity(clean(sus_text)))

# check impurity of the cleaned text overall
print(df := df.assign(clean_text = df["text"].map(clean),
                      impurity = lambda df: df["clean_text"].apply(impurity, min_len = 20)))

print(df[["clean_text", "impurity"]].sort_values(by = "impurity", ascending = False).head(3))

#+END_SRC

* Blueprint: Character Normalization with textacy

#+BEGIN_SRC python
import textacy.preprocessing as tprep

def normalize(text, additional_ops = []):
    operations = [
        tprep.normalize.hyphenated_words,
        tprep.normalize.quotation_marks,
        tprep.normalize.unicode,
        tprep.remove.accents
    ]

    if additional_ops:
        operations.extend(additional_ops)

    for op in operations:
        text = op(text)

    return text

ex_text = "The caf√© ‚ÄúSaint-Rapha√´l‚Äù is loca-\nted on C√¥te d ºAzur. yes@mailbox.org visit http://website.web"

print("\nRegular:\t", ex_text, "\nNormalized:\t", normalize(ex_text, additional_ops = [tprep.replace.emails, tprep.replace.urls]))
#+END_SRC

* Blueprint: Pattern-Based Data Masking with textacy


#+BEGIN_SRC python
# find the most frequently used URLs in the corpus
from textacy.preprocessing.resources import RE_URL

print(count_words(df, column="clean_text", preprocess = RE_URL.findall))

# finalize data cleaning with data masking and normalization
from toolz import compose

print(df := df.assign(clean_text = df["clean_text"].map(compose(normalize, tprep.replace.urls))))

# rename text columns and drop impurity
print(df := df.rename(columns = {"text": "raw_text",
                                 "clean_text": "text"})
      .drop(columns = ["impurity"]))

print(df.columns)

con = sqlite3.connect(db_name)
df.to_sql("posts_cleaned", con, index = False, if_exists = "replace")
con.close()
#+END_SRC

* Blueprint: Tokenization with Regular Expressions

#+BEGIN_SRC python
text = """
2019-08-10 23:32: @pete/@louis - I don't have a well-designed
solution for today's problem. The code of module AC68 should be
-1.
Have to think a bit... #goodnight ;-) üò©üò¨"""

import re

# scikitlearn approach uses \w\w+ for default tokenization
tokens = re.findall(r"\w\w+", text)
print(*tokens, sep = "|")

# with additional expressions for emojis
RE_TOKEN = re.compile(r"""
( [#]?[@\w'‚Äô\.\-\:]*\w        # words, hashtags, and email addresses
| [:;<]\-?[\)\(3]             # coarse pattern for basic text emojis
| [\U0001F100-\U0001FFFF]     # coarse range for unicode emojis
)
""", re.VERBOSE)

def tokenize(text):
    return RE_TOKEN.findall(text)

tokens = tokenize(text)

print(*tokens, sep = "|")
#+END_SRC

* Tokenization with NLTK

#+BEGIN_SRC python
import nltk

tokens = nltk.tokenize.word_tokenize(text)
print(*tokens, sep = "|")
#+END_SRC

* Linguistic Processing with spaCy

** Instantiating a Pipeline

#+BEGIN_SRC python
import spacy

nlp = spacy.load("en_core_web_sm")

print(nlp.pipeline)

# if we only need some parts of the pipeline, we can disable others on load
nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])
#+END_SRC

** Processing Text

#+BEGIN_SRC python
nlp = spacy.load("en_core_web_sm")
text = "My best friend Ryan Peters likes fancy adventure games."
doc = nlp(text)

for token in doc:
    print(token, end = "|")

def display_nlp(doc, include_punct = False):
    """
    Generate data frame for visualization of spaCy tokens.
    """
    rows = []
    for i, t in enumerate(doc):
        if not t.is_punct or include_punct:
            row = {"token": i,
                   "text": t.text,
                   "lemma_": t.lemma_,
                   "is_stop": t.is_stop,
                   "is_alpha": t.is_alpha,
                   "pos_": t.pos_,
                   "dep_": t.dep_,
                   "ent_type_": t.ent_type_,
                   "ent_iob_": t.ent_iob_}
            rows.append(row)
    df = pd.DataFrame(rows).set_index("token")
    df.index.name = None
    return df

print(display_nlp(doc))
#+END_SRC

* Blueprint: Customizing Tokenization

#+BEGIN_SRC python
text = "@Pete: choose low-carb #food #eat-smart. _url_ ;-) üòãüëç"

doc = nlp(text)

for token in doc:
    print(token, end = " | ")

# create a tokenizer with individual rules for infix, prefix, suffix splitting
from spacy.tokenizer import Tokenizer
from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex

def custom_tokenizer(nlp):
    # use default patterns except the ones matched by re.search
    prefixes = [pattern for pattern in nlp.Defaults.prefixes if pattern not in ["-", "_", "#"]]
    infixes = [pattern for pattern in nlp.Defaults.infixes if not re.search(pattern, "xx-xx")]
    suffixes = [pattern for pattern in nlp.Defaults.suffixes if pattern not in ["_"]]

    return Tokenizer(vocab = nlp.vocab,
                     rules = nlp.Defaults.tokenizer_exceptions,
                     prefix_search = compile_prefix_regex(prefixes).search,
                     infix_finditer = compile_infix_regex(infixes).finditer,
                     suffix_search = compile_suffix_regex(suffixes).search,
                     token_match = nlp.Defaults.token_match)

nlp.tokenizer = custom_tokenizer(nlp)

doc = nlp(text)

for token in doc:
    print(token, end = "|")
#+END_SRC

* Blueprint: Working with Stop Words

#+BEGIN_SRC python
text = "Dear Ryan, we need to sit down and talk. Regards, Pete"
doc = nlp(text)

non_stop = [t for t in doc if not t.is_stop and not t.is_punct]

print(non_stop)

# modify list of stop words
nlp = spacy.load("en_core_web_sm")
nlp.vocab["down"].is_stop = False
nlp.vocab["Dear"].is_stop = True
nlp.vocab["Regards"].is_stop = True

# update
doc = nlp(text)

non_stop = [t for t in doc if not t.is_stop and not t.is_punct]

print(non_stop)
#+END_SRC

* Blueprint: Extracting Lemmas Based on Part of Speech

#+BEGIN_SRC python
text = "My best friend Ryan Peters likes fancy adventure games."
doc = nlp(text)

print(*[t.lemma_ for t in doc], sep = "|")

# get just the nouns and proper nouns
nouns = [t for t in doc if t.pos_ in ["NOUN", "PROPN"]]
print(nouns)

print(display_nlp(doc))

# extract adjectives and nouns from the sample sentence
import textacy

tokens = textacy.extract.words(doc, filter_stops = True,
                               filter_punct = True,
                               filter_nums = True,
                               include_pos = ["ADJ", "NOUN"],
                               exclude_pos = None,
                               min_freq = 1)

print(*[t for t in tokens], sep = "|")

def extract_lemmas(doc, **kwargs):
    return [t.lemma_ for t in textacy.extract.words(doc, **kwargs)]

lemmas = extract_lemmas(doc, include_pos = ["ADJ", "NOUN"])

print(*lemmas, sep = "|")
#+END_SRC

* Blueprint: Extracting Noun Phrases

#+BEGIN_SRC python
# extract sequences of nouns with a preceding adjective
text = "My best friend Ryan Peters likes fancy adventure games."
doc = nlp(text)

patterns = ["POS:ADJ POS:NOUN:+"]

spans = textacy.extract.token_matches(doc, patterns = patterns)

print(*[s.lemma_ for s in spans], sep = "|")

# alternatively
print(*doc.noun_chunks, sep = "|")

# search for sequence of nouns preceded by given token and return the lemmas
def extract_noun_phrases(doc, preceding_pos = ["NOUN"], sep = "_"):
    patterns = []
    for pos in preceding_pos:
        patterns.append(f"POS:{pos} POS:NOUN:+")
    spans = textacy.extract.token_matches(doc, patterns = patterns)
    return [sep.join([t.lemma_ for t in s]) for s in spans]

print(*extract_noun_phrases(doc, ["ADJ", "NOUN"]), sep = " | ")
#+END_SRC

* Blueprint: Extracting Named Entities

#+BEGIN_SRC python
text = "James O'Neill, chairman of World Cargo Inc, lives in San Francisco."
doc = nlp(text)

for ent in doc.ents:
    print(f"({ent.text}, {ent.label_})", end = " ")

# visualization
import matplotlib.pyplot as plt
from spacy import displacy
displacy.serve(doc, style = "ent")

def extract_entities(doc, include_types = None, sep = "_"):
    ents = textacy.extract.entities(doc,
                                    include_types = include_types,
                                    exclude_types = None,
                                    drop_determiners = True,
                                    min_freq = 1)
    return [sep.join([t.lemma_ for t in e]) + "/" + e.label_ for e in ents]

print(extract_entities(doc, ["PERSON", "GPE"]))
#+END_SRC

* Blueprint: Creating One Function to Get It All

#+BEGIN_SRC python
def extract_nlp(doc):
    return {
        "lemmas": extract_lemmas(doc,
                                 exclude_pos = ["PART", "PUNCT", "DET", "PRON", "SYM", "SPACE"],
                                 filter_stops = False),
        "adjs_verbs": extract_lemmas(doc, include_pos = ["ADJ", "VERB"]),
        "nouns": extract_lemmas(doc, include_pos = ["NOUN", "PROPN"]),
        "noun_phrases": extract_noun_phrases(doc, ["NOUN"]),
        "adj_noun_phrases": extract_noun_phrases(doc, ["ADJ"]),
        "entities": extract_entities(doc, ["PERSON", "ORG", "GPE", "LOC"])
    }

text = "My best friend Ryan Peters likes fancy adventure games."
doc = nlp(text)

print("\n")
for col, values in extract_nlp(doc).items():
    print(f"{col}:\t\t{values}")

# retrieve the list of returned column names
nlp_columns = list(extract_nlp(nlp.make_doc("")).keys())
print(nlp_columns)
#+END_SRC

* Blueprint: Using spaCy on a Large Dataset

#+BEGIN_SRC python
db_name = "reddit-selfposts.db"
con = sqlite3.connect(db_name)
df = pd.read_sql("select * from posts_cleaned", con)
con.close()

print(df := df.assign(text = df["title"] + ": " + df["text"]))

# initialize new DataFrame columns we wish to fill with values
for col in nlp_columns:
    df[col] = None

# see if we can perform the computation on a GPU
if spacy.prefer_gpu():
    print("Working on GPU")
else:
    print("No GPU found, working on CPU")

# extract the features and place them in a dataframe
batch_size = 50
for i in range(0, len(df), batch_size):
    docs = nlp.pipe(df["text"][i:i+batch_size])
    for j, doc in enumerate(docs):
        for col, values in extract_nlp(doc).items():
            df[col].iloc[i+j] = values

# frequency analysis
count_words(df, "noun_phrases").head(10).plot(kind = "barh").invert_yaxis()
plt.show()

# persist the result (save to DB)
## serialize the extracted lists to space separated strings
df[nlp_columns] = df[nlp_columns].applymap(lambda items: " ".join(items))
con = sqlite3.connect(db_name)
df.to_sql("posts_nlp", con, index = False, if_exists = "replace")
con.close()
#+END_SRC

Rule of Thumb:

In general, you are better off with lemmatized text when you do text classification, topic modeling, or clustering based on TF-IDF.
You should avoid or use sparingly those kinds of normalization or stop word removal for more complex machine learning tasks such as
text summarization, machine translation, or question answering where the model needs to reflect the variety of the language.
