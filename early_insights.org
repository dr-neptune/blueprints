#+TITLE: Chapter 1. Gaining Early Insights from Textual Data
#+PROPERTY: header-args :tangle insights.py

* Libaries

#+BEGIN_SRC python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
#+END_SRC

* Read the Data

#+BEGIN_SRC python
un = pd.read_csv("data/un-debates.csv")

print(un)
#+END_SRC

* Blueprint: Getting an Overview of the Data with Pandas

We will perform the following steps:

1. Calculate summary statistics
2. Check for missing values
3. Plot distributions of interesting attributes
4. Compare distributions across categories
5. Visualize developments over time

#+BEGIN_SRC python
# print column names
print(un.columns.tolist())

# print column data types
print(un.dtypes)

# data types plus memory consumption
print(un.info())

# summary statistics
print(un.describe())

# summary statistics for all columns (including categorical)
print(un.describe())
#+END_SRC

** Calculating Summary Statistics for Columns

Let's add a new numerical column to the dataframe containing the text length to get some additional information about the distribution of the lengths of the speeches.

#+BEGIN_SRC python
# add text length column and describe it
un = un.assign(length = un["text"].str.len())

print(un.assign(length=un["text"].str.len()).describe().T)
print(un.assign(length=un["text"].str.len()).describe(include="O").T)

# check number of unique values for categorical predictors
print(un[["country", "speaker"]].describe(include="O").T)
#+END_SRC

** Checking for Missing Data

#+BEGIN_SRC python
# check how many NA values we have
print(un.isna().sum())

# fill in the NA with "unknown"
# warning: this is a mutable operation
print(un["speaker"].fillna("unknown", inplace = True))
print(un.isna().sum())

# check specific values and their counts
print(un[un["speaker"].str.contains("Bush")]["speaker"].value_counts())
#+END_SRC

** Plotting Value Distributions

#+BEGIN_SRC python
# plot a box and whisker plot and a histogram side by side
plt.subplot(1, 2, 1)
un["length"].plot(kind = "box", vert = False)
plt.subplot(1, 2, 2)
un["length"].plot(kind = "hist", bins = 30)
plt.title("Speech Length (Characters)")
plt.show()


def gen_dist_plot(column_name):
    plt.close()
    # plot a single distribution plot
    sns.displot(un[column_name], kind = "kde")
    sns.rugplot(un[column_name])
    plt.title(column_name.title())
    plt.show()

gen_dist_plot("length")
#+END_SRC

** Comparing Value Distributions Across Categories

A nice visualization to compare distributions across different categories is Seaborn's catplot

#+BEGIN_SRC python
where = un["country"].isin(["USA", "FRA", "GBR", "CHN", "RUS"])

print(un[where])

sns.catplot(data = un[where], x="country", y="length", kind="box")
sns.catplot(data = un[where], x="country", y="length", kind="violin")
plt.show()
#+END_SRC

** Visualizing Developments Over Time

#+BEGIN_SRC python
plt.subplot(1, 2, 1)
un.groupby("year").size().plot(title = "Number of Countries")
plt.subplot(1, 2, 2)
un.groupby("year").agg({"length": "mean"}).plot(title = "Avg Speech Length", ylim = (0, 30000))
plt.show()
#+END_SRC

#+BEGIN_SRC python :tangle ch1/overview.py
# import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


class Overview:
    """
    This class instantiates an object that provides an overview of a data frame.
    Example:

    >> overview = Overview(df)
    # get summary statistics
    >> overview.summary_stats()
    ## check for missing values
    >> overview.check_missing()
    ## generate a specific univariate plot
    >> overview.gen_uni_plot("column_name")
    ## generate all univariate plots
    >> overview.gen_all_unis()
    """

    def __init__(self, df):
        self.df = df

    def summary_stats(self, mem_usage="deep", include="O"):
        """
        Returns a dictionary containing the following summary stats:

        - col names: df.dtype
        - data types + memory consumption: df.info
          - set mem_usage to "" if you don't want to spend more time on "deeper" memory estimates
        - summary: df.describe
          - set include to "" if you don't wish to include categorical variables
        """
        column_names = list(self.df.columns)
        # returns a function. Evaluate to get info.
        ## This is because df.info is just a print side effect
        data_types = lambda: self.df.info(memory_usage=mem_usage)
        summary = self.df.describe(include=include).T

        return {
            "col_names": column_names,
            "data_types": data_types,
            "summary": summary,
        }

    def check_missing(self):
        """
        Returns the counts of missing values in the dataframe
        """
        return self.df.isna().sum()

    def gen_uni_plot(self, column_name):
        """
        Generates a univariate density plot for the given column name. Requires a numeric or datetime column
        """
        new_plot = UnivariatePlot(self.df, column_name)
        new_plot.gen_plot()

    def gen_all_unis(self):
        # the [:-1] is because the text field is too large to fix in the axis labels
        return [self.gen_uni_plot(i) for i in self.summary_stats()["col_names"][:-1]]


# un_overview = Overview(un)
# un_overview.gen_all_unis()


class UnivariatePlot:
    sns.set(palette="colorblind")

    def __init__(self, df, column_name, keep_null=False):
        self.column_name = column_name
        # if you wish to keep the null values, pass True to keep_null
        if keep_null:
            self.df = df[column_name].to_frame()
        else:
            self.df = df[column_name].dropna().to_frame()

    # def gen_dist_plot(self):
    #     """
    #     Generates a univariate density plot for the given column name. Requires a numeric or datetime column
    #     """
    #     plt.close()
    #     # plot a single distribution plot
    #     sns.displot(data=self.df, kind="kde")
    #     sns.rugplot(data=self.df)
    #     plt.title(self.column_name.title())
    #     plt.show()

    def gen_dist_plot_double(self):
        """
        Generates a pair of plots:
        - a box and whisker plot on the left
        - a histogram on the right
        """
        plt.subplot(1, 2, 1)
        self.df[self.column_name].plot(kind="box", vert=False)
        plt.title("Speech Length (Characters)")
        plt.subplot(1, 2, 2)
        self.df[self.column_name].plot(kind="hist", bins=30)
        plt.show()

    def gen_count_plot(self, top_n=10):
        """
        Generates a count plot for the given column name.
        Returns @top_n values ordered by highest cardinality
        """

        plt.close()
        sns.countplot(
            y=self.column_name,
            data=self.df,
            order=self.df[self.column_name].value_counts().iloc[:top_n].index,
        )
        plt.title(self.column_name.title())
        plt.show()

    def gen_plot(self):
        if self.df[self.column_name].dtype == "object":
            self.gen_count_plot()
        elif self.df[self.column_name].dtype in ["int64", "datetime", "float"]:
            self.gen_dist_plot()
        else:
            raise ValueError("Column type not in [object, int64, datetime, float]")


# un_len = UnivariatePlot(un, "length")
# un_position = UnivariatePlot(df=un, column_name="country")
# un_position.gen_plot()
# un_len.gen_plot()
# un_len.gen_dist_plot_double()
#+END_SRC

* Blueprint: Building a Simple Text Preprocessing Pipeline

Our pipeline will look something like this:

source text -> case-folding -> tokenization -> stop word removal -> prepared tokens

** Performing Tokenization with Regular Expressions

#+BEGIN_SRC python
import regex as re

def tokenize(text):
    # \p{L} matches all unicode letters
    return re.findall(r"[\w-]*\p{L}[\w-]*", text)

text = "Let's defeat SARS-CoV-2 together in 2020!"

print("|".join(tokens := tokenize(text)))
#+END_SRC

** Treating Stop Words

#+BEGIN_SRC python
stopwords = set(nltk.corpus.stopwords.words("english"))

def remove_stopwords(tokens):
    return [t for t in tokens if t.lower() not in stopwords]

# adding additional stopwords
include_stopwords = {"dear", "regards", "must", "would", "also"}
exclude_stopwords = {"against"}

stopwords |= include_stopwords
stopwords -= exclude_stopwords
#+END_SRC

** Processing a Pipeline with One Line of Code

#+BEGIN_SRC python
from toolz import compose

pipeline = [str.lower, tokenize, remove_stopwords]

# lol
def prepare(text, pipeline):
    # reverses the pipeline and calls it in reverse-order on the text
    return compose(*pipeline[::-1])(text)

# applying prepare to a dataframe
un = un.assign(tokens = un["text"].apply(prepare, pipeline = pipeline))

print(un[["text", "tokens"]])
#+END_SRC

*** Pandas Higher Order Functions

| Function     | Description                                               |
|--------------+-----------------------------------------------------------|
| Series.map   | Works element by element on a Pandas Series               |
| Series.apply | Same as map but allows additional params                  |
| df.applymap  | rowwise map on a dataframe                                |
| df.apply     | works on rows or columns of a df and supports aggregation |
