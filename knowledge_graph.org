#+TITLE: Chapter 12: Building a Knowledge Graph

#+BEGIN_SRC python
import nltk
nltk.download('reuters')
from nltk.corpus import reuters

df = pd.DataFrame(reuters.fileids('acq'), columns=['fileid'])
df['raw'] = df['fileid'].map(lambda f: reuters.raw(f))
df.index = df['fileid'].map(lambda f: int(f.split('/')[1]))
df.index.name = None
df = df.drop(columns=['fileid']).sort_index()

print(df.sample(3, random_state=42))

df[['headline', 'raw_text']] = df.apply(lambda row: row['raw'].split('\n', 1),
                                        axis='columns', result_type='expand')

def clean(text):
    text = text.replace('&lt;','<') # html escape
    text = re.sub(r'[<>]', '"', text) # quotation marks instead of <>
    text = re.sub(r'[ ]*"[A-Z\.]+"', '', text) # drop stock symbols
    text = re.sub(r'[ ]*\([A-Z\.]+\)', '', text) # drop stock symbols
    text = re.sub(r'\bdlr(s?)\b', r'dollar\1', text, flags=re.I)
    text = re.sub(r'\bmln(s?)\b', r'million\1', text, flags=re.I)
    text = re.sub(r'\bpct\b', r'%', text, flags=re.I)
    # normalize INC to Inc
    text = re.sub(r'\b(Co|Corp|Inc|Plc|Ltd)\b', lambda m: m.expand(r'\1').capitalize(), text, flags=re.I)
    text = re.sub(r'"', r'', text) # quotation marks
    text = re.sub(r'\s+', ' ', text) # multiple whitespace by one
    text = re.sub(r'acquisiton', 'acquisition', text) # typo
    text = re.sub(r'Nippon bLife', 'Nippon Life', text) # typo
    text = re.sub(r'COMSAT.COMSAT', 'COMSAT. COMSAT', text) # missing space at end of sentence
    #text = re.sub(r'Audio/Video', 'Audio-Video', text) # missing space at end of sentence

    return text.strip()

df['text'] = df['raw_text'].map(clean)
df['headline'] = df['headline'].map(clean)

print(df[df['raw_text'].map(lambda t: t.isupper())][['headline', 'raw_text']].head(3))

# drop articles with only capital letters
df = df[df['raw_text'].map(lambda t: not t.isupper())]

# this is our cleaned dataset
print(df[['headline', 'text']].sample(3, random_state=42))
#+END_SRC
* Named Entity Recognition

#+BEGIN_SRC python
import spacy
from spacy import displacy
import matplotlib.pyplot as plt

nlp = spacy.load('en_core_web_sm')

print(*nlp.pipeline, sep='\n')

text = """Hughes Tool Co Chairman W.A. Kistler said its
merger with Baker International Corp was still under consideration.
We hope to come soon to a mutual agreement, Kistler said.
The directors of Baker filed a law suit in Texas to force
Hughes to complete the merger."""

doc = nlp(text)

print(*[(e.text, e.label_) for e in doc.ents], sep=' ')

displacy.serve(doc, style='ent')
#+END_SRC

* Blueprint: Using Rule-Based Named-Entity Recognition

#+BEGIN_SRC python
# pattern match depts of the US govt and the SEC
from spacy.pipeline import EntityRuler

departments = ['Justice', 'Transportation']
patterns = [{"label": "GOV",
             "pattern": [{"TEXT": "U.S.", "OP": "?"},
                         {"TEXT": "Department"}, {"TEXT": "of"},
                         {"TEXT": {"IN": departments}, "ENT_TYPE": "ORG"}]},
             {"label": "GOV",
              "pattern": [{"TEXT": "U.S.", "OP": "?"},
                          {"TEXT": {"IN": departments}, "ENT_TYPE": "ORG"},
                          {"TEXT": "Department"}]},
             {"label": "GOV",
              "pattern": [{"TEXT": "Securities"}, {"TEXT": "and"},
                          {"TEXT": "Exchange"}, {"TEXT": "Commission"}]}]

entity_ruler = EntityRuler(nlp, patterns=patterns, overwrite_ents=True)
nlp.add_pipe('entity_ruler')

text = """Justice Department is an alias for the U.S. Department of Justice.
Department of Transportation and the Securities and Exchange Commission
are government organisations, but the Sales Department is not."""

doc = nlp(text)
displacy.serve(doc, style='ent')
#+END_SRC

* Blueprint: Normalizing Named Entities

One approach to simplify the resolution of different entity mentions to a single name is the normalization or standardization of mentions.

#+BEGIN_SRC python
from spacy.tokens import Span
from spacy.language import Language
from spacy.pipeline import merge_entities

text = "Baker International's shares climbed on the New York Stock Exchange."

doc = nlp(text)

print(*[([t.text for t in e], e.label_) for e in doc.ents], sep='\n')

@Language.component('norm_entities')
def norm_entities(doc):
    ents = []
    for ent in doc.ents:
        if ent[0].pos_ == "DET": # leading article
            ent = Span(doc, ent.start+1, ent.end, label=ent.label)
        if len(ent) > 0:
            if ent[-1].pos_ == "PART": # trailing particle like 's
                ent = Span(doc, ent.start, ent.end-1, label=ent.label)
            if len(ent) > 0:
                ents.append(ent)
    doc.ents = tuple(ents)
    return doc

nlp.add_pipe('norm_entities')

doc = nlp(text)

print(*[([t.text for t in e], e.label_) for e in doc.ents], sep='\n')

# merging entity tokens
nlp.add_pipe('merge_entities')

doc = nlp(text)

print(*[([t.text for t in e], e.label_) for e in doc.ents], sep='\n')

#+END_SRC

* Coreference Resolution

One of the greatest obstacles in information extraction is the fact that entity mentions appear in many different spellings (also called surface forms).

Coreference Resolution is the task of determining the different mentions of an entity within a single text, for example: abbreviated names, aliases, or pronouns.
The result of this step is a group of coreferencing mentions called a mention cluster.

* Blueprint: Using spaCy's Token Extensions

We need a way to create a link from the different mentions of an entity to the main reference, the referent.
After coreference resolution, the token for Kistler of the example article should point to (W.A. Kistler, PERSON)

spaCy's extension mechanism allows us to define custom attributes, and this is the perfect way to store this kind of information with tokens.

#+BEGIN_SRC python
from spacy.tokens import Token
Token.set_extension('ref_n', default='')  # ref name
Token.set_extension('ref_t', default='')  # ref type

def init_coref(doc):
    '''Ensures that each entity mention of type org, gov, person gets an initial reference to itself'''
    for e in doc.ents:
        if e.label_ in ['ORG', 'GOV', 'PERSON']:
            e[0]._._ref_n, e[0]._.ref_t = e.text, e.label_
    return doc
#+END_SRC

* Blueprint: Performing Alias Resolution

#+BEGIN_SRC python
# acronyms created after cooccurrence analysis
_acronyms = {
    'AMC': 'American Motors Corp',
    'AMI': 'American Medical International Inc',
    'ARCO': 'Atlantic Richfield Co',
    'BIL': 'Brierley Investments Ltd',
    'BP': 'British Petroleum Co Plc',
    'BPI': 'Banco Portugues de Investmento Sarl',
    'CGCT': 'Cie Generale de Constructions',
    'DAF': 'Delhi Australia Fund',
    'EC': 'European Community',
    'ETL': 'Equiticorp Tasman Ltd',
    'FCC': 'Federal Communications Commission',
    'FDA': 'Food and Drug Administration',
    'FHLBB': 'Federal Home Loan Bank Board',
    'FIRB': 'Foreign Investment Review Board',
    'FTC': 'Federal Trade Commission',
    'ICC': 'Interstate Commerce Commission',
    'IDC': 'International Digital Communications Planning Inc',
    'ITJ': 'International Telecom Japan Inc',
    'KDD': 'Kokusai Denshin Denwa Co Ltd',
    'PCGG': 'Presidential Commission on Good Government',
    'PSA': 'Pacific Southwest Airlines',
    'SMC': 'San Miguel Corp',
    'TWA': 'Trans World Airlines Inc',
    'UCPB': 'United Coconut Planters Bank'
}

# add acronyms (all acronyms are organizations)
alias_lookup = {acro: (text, 'ORG') for (acro, text) in _acronyms.items()}

alias_lookup['SEC'] = ('Securities and Exchange Commission', 'GOV')

alias_list = {('U.S. Department of Justice', 'GOV'):
                ['U.S. Department of Justice',
                 'Department of Justice',
                 'U.S. Justice Department',
                 'Justice Department'],
              ('U.S. Department of Transportation', 'GOV'):
                ['U.S. Department of Transportation',
                 'U.S. Transportation Department',
                 'Department of Transportation',
                 'Transportation Department',
                 'DOT'],
              ('USAir Group Inc', 'ORG'):
                ['USAir Group Inc', 'USAir Group Inc.',
                 'US Air Corp', 'US Air Corp.',
                 'USAir Group', 'USAir Group Inc', 'USAir Group Inc.',
                 'US Air', 'USAir', 'U.S. Air', 'USAIR Group',
                 'U.S. Air Group Inc.'],
              ('Trans World Airlines Inc', 'ORG'):
                ['Transworld Airlines', 'Transworld Airlines Inc', 'Trans World Airlines'],
}

# invert alias_list; overwrites entries in acronyms like DOT
alias_lookup.update({alias: ent for (ent, aliases) in alias_list.items()
                                for alias in aliases})

for token in ['Transportation Department', 'DOT', 'SEC', 'TWA']:
    print(token, ':', alias_lookup[token])

@Language.component('alias_resolver')
def alias_resolver(doc):
    """lookup aliases and store results in ref_t, ref_n"""
    for ent in doc.ents:
        token = ent[0].text
        if token in alias_lookup:
            a_name, a_type = alias_lookup[token]
            ent[0]._.ref_n, ent[0]._.ref_t = a_name, a_type
    return propagate_ent_type(doc)


def propagate_ent_type(doc):
    """propagate entity type stored in ref_t"""
    ents = []
    for e in doc.ents:
        if e[0]._.ref_n != '':  # if e is a coreference
            e = Span(doc, e.start, e.end, label=e[0]._.ref_t)
            ents.append(e)
    doc.ents = tuple(ents)
    return doc

nlp.add_pipe('alias_resolver')

def display_ner(doc, include_punct=False):
    """Generate data frame for visualization of spaCy doc with custom attributes."""

    rows = []
    for i, t in enumerate(doc):
        if not t.is_punct or include_punct:
            row = {'token': i,
                   'text': t.text, 'lemma': t.lemma_,
                   'pos': t.pos_, 'dep': t.dep_, 'ent_type': t.ent_type_,
                   'ent_iob_': t.ent_iob_}

            if doc.has_extension('has_coref'):
                if doc._.coref_clusters is not None and \
                   t.has_extension('in_coref') and t._.in_coref: # neuralcoref attributes
                    row['in_coref'] = t._.in_coref
                    row['main_coref'] = t._.coref_clusters[0].main.text
                else:
                    row['in_coref'] = None
                    row['main_coref'] = None
            if t.has_extension('ref_n'): # referent attribute
                row['ref_n'] = t._.ref_n
                row['ref_t'] = t._.ref_t
            if t.has_extension('ref_ent'): # ref_n/ref_t
                row['ref_ent'] = t._.ref_ent
            rows.append(row)

    df = pd.DataFrame(rows).set_index('token')
    df.index.name = None

    return df

text = """
The deal of Trans World Airlines is under investigation by the
U.S. Department of Transportation.
The Transportation Department will block the deal of TWA.
"""

doc = nlp(text)

print(display_ner(doc).query("ref_n != ''")[['text', 'ent_type', 'ref_n', 'ref_t']])
#+END_SRC

* Blueprint: Resolving Name Variations

Alias resolution only works if the aliases are known up front.

#+BEGIN_SRC python
def reset_pipeline(nlp, pipes):
    # remove all custom pipes
    custom_pipes = [pipe for (pipe, _) in nlp.pipeline
                    if pipe not in ['tagger', 'parser', 'ner',
                                    'tok2vec', 'attribute_ruler', 'lemmatizer']]
    for pipe in custom_pipes:
        _ = nlp.remove_pipe(pipe)
    # re-add specified pipes
    for pipe in pipes:
        if 'neuralcoref' == pipe or 'neuralcoref' in str(pipe.__class__):
            nlp.add_pipe(pipe, name='neural_coref')
        else:
            nlp.add_pipe(pipe)

    print(f"Model: {nlp.meta['name']}, Language: {nlp.meta['lang']}")
    print(*nlp.pipeline, sep='\n')

# reset_pipeline(nlp, ['entity_ruler', 'norm_entities', 'merge_entities', 'init_coref', 'alias_resolver'])

# define a simple rule for name matching: a seconary mention matches a primary mention if all of its words appear in the primary mention in the same order
def name_match(m1, m2):
    m2 = re.sub(r'[()\.]', '', m2)  # ignore parens and dots
    m2 = r'\b' + m2 + r'\b'  # \b marks word boundary
    m2 = re.sub(r'\s+', r'\\b.*\\b', m2)
    return re.search(m2, m1, flags=re.I) is not None


@Language.component('name_resolver')
def name_resolver(doc):
    """create name-based reference to e1 as primary mention of e2"""
    ents = [e for e in doc.ents if e.label_ in ['ORG', 'PERSON']]
    for i, e1 in enumerate(ents):
        for e2 in ents[i+1:]:
            if name_match(e1[0]._.ref_n, e2[0].text):
                e2[0]._.ref_n = e1[0]._.ref_n
                e2[0]._.ref_t = e1[0]._.ref_t
    return propagate_ent_type(doc)

nlp.add_pipe('name_resolver')

text = """
Hughes Tool Co Chairman W.A. Kistler said its merger with
Baker International Corp. was still under consideration.
We hope to come to a mutual agreement, Kistler said.
Baker will force Hughes to complete the merger.
"""

doc = nlp(text)

displacy.serve(doc, style='ent')

print(display_ner(doc))
#+END_SRC

* Blueprint: Performing Anaphora Resolution with NeuralCoref

In linguistics, /anaphora/ are words whose interpretation depends on the preceding text.

NeuralCoref from huggingface is a library for resolving these kinds of coreferences.
The algorithm uses feature vectors on word embeddings in combination with two neural networks to identify coreference clusters and their main mentions.

#+begin_src python
import spacy
spacy.prefer_gpu()
nlp = spacy.load("en_core_web_sm")

from neuralcoref import NeuralCoref

neural_coref = NeuralCoref(nlp.vocab, greedyness=0.45)
nlp.add_pipe(neural_coref, name='neural_coref')

doc = nlp(text)
print(*doc._.coref_clusters, sep='\n')

def anaphor_coref(doc):
    """anaphora resolution"""
    for token in doc:
        # if token is coref and not already dereferenced
        if token._.in_coref and token._.ref_n == '':
            ref_span = token._.coref_clusters[0].main  # get referred span
            if len(ref_span) <= 3:  # consider only short spans
                for ref in ref_span:  # find first dereferenced entity
                    if ref._.ref_n != '':
                        token._.ref_n = ref._.ref_n
                        token._.ref_t = ref._.ref_t
                        break
    return doc

# add resolved to pipeline and check result
nlp.add_pipe(anaphor_coref)
doc = nlp(text)
display_ner(doc).query("main_coref != ''")
#+end_src

* Blueprint: Name Normalization

Even though our name resolution unifies company mentions within an article, the company names are still inconsistent across articles. We can harmonize company mentions by removing the legal suffixes like Co. or Inc. from company names.

#+begin_src python
def strip_legal_suffix(text):
    return re.sub(r'(\s+and)?(\s+|\b(Co|Corp|Inc|Plc|Ltd)\b\.?)*$',
                  '', text)

print(strip_legal_suffix('Hughes Tool Co'))

def norm_names(doc):
    for t in doc:
        if t._.ref_n != '' and t._.ref_t in ['ORG']:
            t._.ref_n = strip_legal_suffix(t._.ref_n)
            if t._.ref_n == '':
                t._.ref_t = ''
    return doc

nlp.add_pipe(norm_names)
#+end_src

* Blueprint: Creating a Co-Occurence Graph

A co-occurence graph is the simplest form of a knowledge graph. The nodes in the graph are entities, e.g. organizations. Two entities share an (undirected) edge if they are mentioned in the same context (e.g. a paragraph or sentence).

** Extracting Co-Occurences from a Document

   #+begin_src python
from itertools import combinations

def extract_coocs(doc, include_types):
    ents = set([(e[0]._.ref_n, e[0]._.ref_t)
                for e in doc.ents if e[0]._.ref_t
                in include_types])
    yield from combinations(sorted(ents), 2)

# use spacy streaming by calling nlp.pipe
batch_size = 100
coocs = []
for i in range(0, len(df), batch_size):
    docs = nlp.pipe(df['text'][i:i+batch_size],
                    disable=['neural_coref',
                             'anaphor_coref'])
    for j, doc in enumerate(docs):
        coocs.extend([(df.index[i+j], *c)
                      for c in extract_coocs(doc, ['ORG', 'GOV'])])

print(*coocs[:3], sep='\n')

coocs = [([id], *e1, *e2) for (id. e1, e2) in coocs]
cooc_df = pd.DataFrame.from_records(coocs,
                                    columns=('article_id', 'ent1', 'type1', 'ent2', 'type2'))
cooc_df = (cooc_df
           .groupby(['ent1', 'type1', 'ent2', 'type2'])
           ['article_id']
           .agg(['count', 'sum'])
           .rename(columns={'count': 'freq',
                            'sum': 'articles'})
           .reset_index()
           .sort_values('freq', ascending=False))
cooc_df['articles'] = cooc_df['articles'].map(lambda lst: ','.join([str(a) for a in lst[:5]]))

# 3 most frequent entity pairs found in the corpus
cooc_df.head(3)
   #+end_src

** Visualizing the Graph with Gephi

We will use Gephi because it is interactive. In order to get the data in the format required for Gephi, we first use networkx to save the list of nodes and edges of the graph in Graph Exchange XML format.

   #+begin_src python
import networkx as nx

graph = nx.from_pandas_edgelist(cooc_df[['ent1', 'ent2', 'articles', 'freq']]
                                .query('freq > 3')
                                .rename(columns={'freq': 'weight'}),
                                source='ent1', target='ent2', edge_attr=True)

nx.readwrite.write_gexf(graph, 'cooc.gexf', encoding='utf-8', prettyprint=True, version='1.2draft')
   #+end_src

* Relation Extraction

In this section, we look at 2 different blueprints for pattern-based relation extraction.

The first (and simpler) blueprint searches for token phrases of the form "subject-predicate-object"
The second uses the syntactical structure of a sentence, the dependency tree, to get more precise results at the price of more complex rules.

In the end, we will generate a knowledge graph based on the four relations: acquires, sells, subsidiary-of, chairperson-of

* Blueprint: Extracting Relations Using Phrase Matching

The first blueprint works like rule-based entity recognition: it tries to identify relations based on patterns for token sequences.

Not working here, check the docs or a different tutorial

  #+begin_src python
from spacy.matcher import Matcher

matcher = Matcher(nlp.vocab)

acq_synonyms = ['acquire', 'buy', 'purchase']

pattern = [{'_': {'ref_t': 'ORG'}}, # subject
           {'_': {'ref_t': {'NOT_IN': ['ORG']}}, 'OP': '*'},
           {'POS': 'VERB', 'LEMMA': {'IN': acq_synonyms}},
           {'_': {'ref_t': {'NOT_IN': ['ORG']}}, 'OP': '*'},
           {'_': {'ref_t': 'ORG'}}] # object

matcher.add('acquires', None, pattern)

subs_synonyms = ['subsidiary', 'unit']

pattern = [{'_': {'ref_t': 'ORG'}}, # subject
           {'_': {'ref_t': {'NOT_IN': ['ORG']}},
            'POS': {'NOT_IN': ['VERB']}, 'OP': '*'},
           {'LOWER': {'IN': subs_synonyms}}, {'TEXT': 'of'},
           {'_': {'ref_t': {'NOT_IN': ['ORG']}},
            'POS': {'NOT_IN': ['VERB']}, 'OP': '*'},
           {'_': {'ref_t': 'ORG'}}] # object

matcher.add('subsidiary-of', None, pattern)

text = """
Fujitsu plans to acquire 80% of Fairchild Corp, an industrial unit of Schlumberger.
"""

def extract_rel_match(doc, matcher):
    for sent in doc.sents:
        for match_id, start, end in matcher(sent):
            span = sent[start:end]
            pred = nlp.vocab.strings[match_id]
            subj, obj = span[0], span[-1]
            if pred.startswith('rev-'):  # reversed relation
                subj, obj = obj, subj
                pred = pred[4:]
            yield ((subj._.ref_n, subj._.ref_t), pred,
                   (obj._.ref_n, obj._.ref_t))

pattern = [{'_': {'ref_t': 'ORG'}}, # subject
           {'LOWER': {'IN': subs_synonyms}}, # predicate
           {'_': {'ref_t': 'ORG'}}] # object

matcher.add('rev-subsidiary-of', None, pattern)

doc = nlp(text)

print(*extract_rel_match(doc, matcher), sep='\n')
  #+end_src

* Blueprint: Extracting Relations Using Dependency Trees

Not working here, check the docs or a different tutorial

  #+begin_src python
text = "Fujitsu, a competitor of NEC, acquired Fairchild Corp."

doc = nlp(text)

displacy.serve(doc, style='dep', options={'compact': False,
                                          'distance': 100})

def extract_rel_dep(doc, pred_name, pred_synonyms, excl_prepos=[]):
    for token in doc:
        if token.pos_ == 'VERB' and token.lemma_ in pred_synonyms:
            pred = token
            passive = is_passive(pred)
            subj = find_subj(pred, 'ORG', passive)
            if subj is not None:
                obj = find_obj(pred, 'ORG', excl_prepos)
                if obj is not None:
                    if passive: # switch roles
                        obj, subj = subj, obj
                    yield ((subj._.ref_n, subj._.ref_t),
                           pred_name, (obj._.ref_n, obj._.ref_t))
  #+end_src

* Creating the Knowledge Graph

Now we can put everything together and create a knowledge graph from our entire corpus.
We can extract organizations, persons, and "acquires", "sells", "subsidiary-of", "executive-of"

  #+begin_src python
nlp = spacy.load('en_core_web_lg')

# add pipes
pipes = [entity_ruler, norm_entities, merge_entities,
         init_coref, alias_resolver, name_resolver,
         neural_coref, anaphor_coref, norm_names]

for pipe in pipes:
    nlp.add_pipe(pipe)

# add 2 additional rules for "executive-of" and "subsidiary-of"
ceo_synonyms = ['chairman', 'president', 'director', 'ceo', 'executive']

pattern = [{'ENT_TYPE': 'PERSON'},
           {'ENT_TYPE': {'NOT_IN': ['ORG', 'PERSON']}, 'OP': '*'},
           {'LOWER': {'IN': ceo_synonyms}}, {'TEXT': 'of'},
           {'ENT_TYPE': {'NOT_IN': ['ORG', 'PERSON']}, 'OP': '*'},
           {'ENT_TYPE': 'ORG'}]

matcher.add('executive-of', None, pattern)

pattern = [{'ENT_TYPE': 'ORG'},
           {'LOWER': {'IN': ceo_synonyms}},
           {'ENT_TYPE': 'PERSON'}]

matcher.add('rev-executive-of', None, pattern)

# define one function to extract all relationships
def extract_rels(doc):
    yield from extract_rel_match(doc, matcher)
    yield from extract_rel_dep(doc, 'acquires', acq_synonyms, ['to', 'from'])
    yield from extract_rel_dep(doc, 'sells', ['sell'], ['to', 'from'])

# extract the relations, convert them into a networkx graph, store the graph in a gexf file for Gephi
  #+end_src
