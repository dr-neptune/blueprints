#+TITLE: Chapter 8: Unsupervised Methods: Topic Modeling and Clustering

* Out Dataset: UN General Debates

#+BEGIN_SRC python
import pandas as pd
import re
import matplotlib.pyplot as plt


df = pd.read_csv('data/un-debates.csv')
print(df.info())

# clean up
df['paragraphs'] = df['text'].map(lambda text: re.split('[.?!]\s*\n', text))
df['number_of_paragraphs'] = df['paragraphs'].map(len)

df.groupby('year').agg({'number_of_paragraphs': 'mean'}).plot.bar()
plt.show()
#+END_SRC

* Preparations

We will be semantically analyzing the corpus of the UN general debates.

#+BEGIN_SRC python
from sklearn.feature_extraction.text import TfidfVectorizer
from spacy.lang.en.stop_words import STOP_WORDS as stopwords

# calculate DTM with bigrams
tfidf_text = TfidfVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)
vectors_text = tfidf_text.fit_transform(df['text'])
print(vectors_text.shape)

# flatten the paragraphs keeping the years
paragraph_df = pd.DataFrame([{'text': paragraph,
                              'year': year} for paragraphs, year in zip(df['paragraphs'], df['year']) for paragraph in paragraphs if paragraph])

tfidf_para_vectorizer = TfidfVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)
tfidf_para_vectors = tfidf_para_vectorizer.fit_transform(paragraph_df['text'])
print(tfidf_para_vectors.shape)
#+END_SRC

* Non-negative Matrix Factorization (NMF)

#+BEGIN_SRC python
from sklearn.decomposition import NMF

nmf_text_model = NMF(n_components=10, random_state=42, max_iter=5000)
W_text_matrix = nmf_text_model.fit_transform(vectors_text)
H_text_matrix = nmf_text_model.components_

def display_topics(model, features, n_top_words=5):
    for topic, word_vector in enumerate(model.components_):
        total = word_vector.sum()
        largest = word_vector.argsort()[::-1]
        print("\nTopic %02d" % topic)
        for i in range(0, n_top_words):
            print("%s (%2.2f)" % (features[largest[i]], word_vector[largest[i]] * 100.0 / total))

display_topics(nmf_text_model,
               tfidf_text.get_feature_names())

# check how many documents could be assigned mainly to each topic
print(W_text_matrix.sum(axis=0) / W_text_matrix.sum() * 100.0)
#+END_SRC

* Blueprint: Creating a Topic Model for Paragraphs Using NMF

#+BEGIN_SRC python
nmf_para_model = NMF(n_components=10, random_state=42)
W_para_matrix = nmf_para_model.fit_transform(tfidf_para_vectors)
H_para_matrix = nmf_para_model.components_

display_topics(nmf_para_model, tfidf_para_vectorizer.get_feature_names())
#+END_SRC

* Blueprint: Creating a Topic Model for Paragraphs with Singular Value Decomposition

#+BEGIN_SRC python
from sklearn.decomposition import TruncatedSVD

svd_para_model = TruncatedSVD(n_components=10, random_state=42)
W_svd_para_matrix = svd_para_model.fit_transform(tfidf_para_vectors)
H_svd_para_matrix = svd_para_model.components_

display_topics(svd_para_model, tfidf_para_vectorizer.get_feature_names())

# determine the sizes of the topics
print(svd_para_model.singular_values_)
#+END_SRC

* Blueprint: Creating a Topic Model for Paragraphs with Latent Dirichlet Allocation

#+BEGIN_SRC python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import pyLDAvis.sklearn

count_para_vectorizer = CountVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)
count_para_vectors = count_para_vectorizer.fit_transform(paragraph_df['text'])

lda_para_model = LatentDirichletAllocation(n_components=10, random_state=42)
W_lda_para_matrix = lda_para_model.fit_transform(count_para_vectors)
H_lda_para_matrix = lda_para_model.components_

display_topics(lda_para_model,
               tfidf_para_vectorizer.get_feature_names())

lda_display = pyLDAvis.sklearn.prepare(lda_para_model, count_para_vectors, count_para_vectorizer, sort_topics=False)

# only works in ipynb
pyLDAvis.display(lda_display)
#+END_SRC

* Blueprint: Using Word Clouds to Display and Compare Topic Models

#+BEGIN_SRC python
import matplotlib.pyplot as plt
from wordcloud import WordCloud

def wordcloud_topics(model, features, no_top_words=40):
    for topic, words in enumerate(model.components_):
        size = {}
        largest = words.argsort()[::-1]  # invert sort order
        for i in range(0, no_top_words):
            size[features[largest[i]]] = abs(words[largest[i]])
            wc = WordCloud(background_color='white',
                           max_words=100,
                           width=960, height=540)
            wc.generate_from_frequencies(size)
            plt.figure(figsize=(12, 12))
            plt.imshow(wc, interpolation='bilinear')
            plt.axis('off')

wordcloud_topics(nmf_para_model,
                 tfidf_para_vectorizer.get_feature_names())

w = wordcloud_topics(lda_para_model,
                     count_para_vectorizer.get_feature_names())

w.show()
#+END_SRC

* Blueprint: Calculating Topic Distribution of Documents and Time Evolution

#+BEGIN_SRC python
import numpy as np
before_1990 = np.array(paragraph_df['year'] < 1990)
after_1990 = ~ before_1990

# calc respective W matrices
W_para_matrix_early = nmf_para_model.transform(tfidf_para_vectors[before_1990])
W_para_matrix_late = nmf_para_model.transform(tfidf_para_vectors[after_1990])
print(W_para_matrix_early.sum(axis=0) / W_para_matrix_early.sum() * 100.0)
print(W_para_matrix_late.sum(axis=0) / W_para_matrix_late.sum() * 100.0)

# calc at the individual year level
year_data = []
years = np.unique(paragraph_years)
for year in tqdm(years):
    W_year = nmf_para_model.transform(tfidf_para_vectors[paragraph_years == year])
    year_data.append([year] + list(W_year.sum(axis=0) / W_year.sum() * 100.0))

topic_names = []
voc = tfidf_para_vectorizer.get_feature_names()
for topic in nmf_para_model.components_:
    important = topic.argsort()
    top_word = voc[important[-1]] + ' ' + voc[important[-2]]
    topic_names.append("Topic " + top_word)

# visualize
df_year = pd.DataFrame(year_data, columns=["year"] + topic_names).set_index("year")
df_year.plot.area()
#+END_SRC

* Blueprint: Preparing Data for Gensim

#+BEGIN_SRC python
from gensim.corpora import Dictionary
from gensim.models import TfidfModel


gensim_paragraphs = [[w for w in re.findall(r'\b\w\w+\b', paragraph.lower()) if w not in stopwords]
                     for paragraph in paragraph_df["text"]]

dict_gensim_para = Dictionary(gensim_paragraphs)

# filter values
dict_gensim_para.filter_extremes(no_below=5, no_above=0.7)

bow_gensim_para = [dict_gensim_para.doc2bow(paragraph) for paragraph in gensim_paragraphs]

# fit model
tfidf_gensim_para = TfidfModel(bow_gensim_para)
vectors_gensim_para = tfidf_gensim_para[bow_gensim_para]
#+END_SRC

* Blueprint: Performing Non-negative Matrix Factorization with Gensim

#+BEGIN_SRC python
from gensim.models.nmf import Nmf
from gensim.models.coherencemodel import CoherenceModel

nmf_gensim_para = Nmf(vectors_gensim_para,
                      num_topics=10,
                      id2word=dict_gensim_para,
                      kappa=0.1,
                      eval_every=5)

print(nmf_gensim_para.show_topics())

# calculate coherence score
nmf_gensim_para_coherence = CoherenceModel(model=nmf_gensim_para,
                                           texts=gensim_paragraphs,
                                           dictionary=dict_gensim_para,
                                           coherence='c_v')

nmf_gensim_para_coherence_score = nmf_gensim_para_coherence.get_coherence()

print(nmf_gensim_para_coherence_score)
#+END_SRC

* Blueprint: Using LDA with Gensim

#+BEGIN_SRC python
from gensim.models import LdaModel

lda_gensim_para = LdaModel(corpus=bow_gensim_para,
                           id2word=dict_gensim_para,
                           chunksize=2000,
                           alpha='auto',
                           eta='auto',
                           iterations=400,
                           num_topics=10,
                           passes=20,
                           eval_every=None,
                           random_state=42)

print(lda_gensim_para.show_topics())
print(lda_gensim_para.log_perplexity(vectors_gensim_para))
#+END_SRC

* Blueprint: Finding the Optimal Number of Topics

The "quality" of a topic model can be measured by the coherence score.

#+BEGIN_SRC python
from gensim.models.ldamulticore import LdaMulticore

lda_para_model_n = []

for n in tqdm(range(5, 21)):
    lda_model = LdaMulticore(corpus=bow_gensim_para,
                             id2word=dict_gensim_para,
                             chunksize=2000,
                             eta='auto',
                             iterations=400,
                             num_topics=n,
                             passes=20,
                             eval_every=None,
                             random_state=42)

    lda_coherence = CoherenceModel(model=lda_model,
                                   texts=gensim_paragraphs,
                                   dictionary=dict_gensim_para,
                                   coherence='c_v')
    lda_para_model_n.append((n, lda_model, lda_coherence.get_coherence()))

pd.DataFrame(lda_para_model_n, columns=["n", "model", "coherence"]).set_index("n")
[["coherence"]].plot(figsize=(16, 9))
plt.show()
#+END_SRC

* Blueprint: Creating a Hierarchical Dirichlet Process with Gensim

The hierarchical topic model should give us just a few broad topics that are well separated, then go into more detail by adding more words and getting more differentiated topic definitions.

#+BEGIN_SRC python
from gensim.models import HdpModel

hdp_gensim_para = HdpModel(corpus=bow_gensim_para,
                           id2word=dict_gensim_para)

print(hdp_gensim_para.print_topics(num_words=10))
#+END_SRC

* Blueprint: Using Clustering to Uncover the Structure of Text Data

#+BEGIN_SRC python
from sklearn.cluster import KMeans

k_means_text = KMeans(n_clusters=10, random_state=42)
k_means_text.fit(tfidf_para_vectors)

# check distribution of words in clusters
print(np.unique(k_means_para.labels_, return_counts=True))

# visualize cluster sizes
sizes = []
for i in range(10):
    sizes.append({"cluster": i,
                  "size": np.sum(k_means_para.labels == i)})

pd.DataFrame(sizes).set_index("cluster").plot.bar(figsize=(16, 9))


# visualize cluster contents
def wordcloud_clusters(model, vectors, features, no_top_words=40):
    for cluster in np.unique(model.labels_):
        size = {}
        words = vectors[model.labels_ == cluster].sum(axis=0).A[0]
        largest = words.argsort()[::-1]
        for i in range(0, no_top_words):
            size[features[largest[i]]] = abs(words[largest[i]])
            wc = WordCloud(background_color="white",
                           max_words=100,
                           width=960,
                           height=540)
            wc.generate_from_frequencies(size)
            plt.figure(figsize=(12,12))
            plt.imshow(wc, interpolation='bilinear')
            plt.axis("off")
            plt.savefig(f'cluster_{cluster}.png')

wordcloud_clusters(k_means_para,
                   tfidf_para_vectors,
                   tfidf_para_vectorizer.get_feature_names())
#+END_SRC
